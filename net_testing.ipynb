{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing our custom functions / classes\n",
    "from helper import multi_acc, CustomMNISTDataset, Net\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'training_data.p'\n",
    "\n",
    "train_df = pd.read_csv('digit-recognizer/train.csv')\n",
    "train_labels = train_df['label']\n",
    "\n",
    "### loading the data if it's been previously generated ###\n",
    "try:\n",
    "    infile = open(filename,'rb')\n",
    "    train_input = pickle.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "### Generating and saving the data ###\n",
    "except:\n",
    "    train_df.drop(['label'],inplace=True, axis=1)\n",
    "    np_train = train_df.to_numpy()\n",
    "    \n",
    "    two_d = np.reshape(np_train[0],[1,28,28])\n",
    "    train_input = two_d\n",
    "\n",
    "    for ind in tqdm(range(1,np.shape(np_train)[0])):\n",
    "    #     if ind == 10:\n",
    "    #         break\n",
    "        two_d = np.reshape(np_train[ind],[1,28,28])\n",
    "        train_input = np.concatenate((train_input, two_d), axis = 0)\n",
    "\n",
    "    print(np.shape(train_input))\n",
    "\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(train_input, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    print('saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net params len: 10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(f'net params len: {len(params)}')\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0258,  0.0025, -0.0742,  0.0788, -0.0329, -0.0876, -0.0658,  0.0724,\n",
      "         -0.1265, -0.0228]], grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlew/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 28, 28)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net conv1.bias.grad before backward tensor([0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('net conv1.bias.grad before backward',net.conv1.bias.grad)\n",
    "\n",
    "# loss.backward()\n",
    "\n",
    "# print('conv1.bias.grad after backward')\n",
    "# print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_input, train_labels.values, test_size=0.20, random_state=42)\n",
    "\n",
    "train_DS = CustomMNISTDataset(labels = torch.from_numpy(y_train).long(), imgs = X_train)\n",
    "test_DS = CustomMNISTDataset(labels = torch.from_numpy(y_test).long(), imgs = X_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_DS, batch_size=50)\n",
    "val_loader = torch.utils.data.DataLoader(test_DS, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:25<03:47, 25.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 0.23678 | Val Loss: 0.28656 | Train Acc: 92.580| Val Acc: 93.238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:53<03:30, 26.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002: | Train Loss: 0.11699 | Val Loss: 0.11876 | Train Acc: 96.750| Val Acc: 97.107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:11<02:45, 23.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003: | Train Loss: 0.09595 | Val Loss: 0.12032 | Train Acc: 97.467| Val Acc: 96.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:28<02:09, 21.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004: | Train Loss: 0.09826 | Val Loss: 0.11315 | Train Acc: 97.327| Val Acc: 97.405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:44<01:40, 20.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005: | Train Loss: 0.10182 | Val Loss: 0.11692 | Train Acc: 97.396| Val Acc: 97.143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [02:01<01:16, 19.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006: | Train Loss: 0.09699 | Val Loss: 0.09847 | Train Acc: 97.643| Val Acc: 97.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [02:18<00:55, 18.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007: | Train Loss: 0.08728 | Val Loss: 0.09660 | Train Acc: 97.881| Val Acc: 97.607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [02:35<00:35, 17.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008: | Train Loss: 0.09193 | Val Loss: 0.09874 | Train Acc: 97.902| Val Acc: 97.917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:53<00:18, 18.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009: | Train Loss: 0.08733 | Val Loss: 0.17248 | Train Acc: 97.952| Val Acc: 97.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:20<00:00, 20.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Train Loss: 0.11715 | Val Loss: 0.13383 | Train Acc: 97.479| Val Acc: 97.048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# a lot of this came from: https://towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab\n",
    "\n",
    "print(\"Begin training.\")\n",
    "for e in tqdm(range(1, n_epochs+1)):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    net.train()\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_train_pred = net(X_train_batch)\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch)\n",
    "        train_acc = multi_acc(y_train_pred, y_train_batch)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "        \n",
    "        \n",
    "    # VALIDATION    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_acc = 0\n",
    "        \n",
    "        net.eval()\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            \n",
    "            y_val_pred = net(X_val_batch)\n",
    "                        \n",
    "            val_loss = criterion(y_val_pred, y_val_batch)\n",
    "            val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "            \n",
    "            val_epoch_loss += val_loss.item()\n",
    "            val_epoch_acc += val_acc.item()\n",
    "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "    loss_stats['val'].append(val_epoch_loss/len(val_loader))\n",
    "    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
    "    accuracy_stats['val'].append(val_epoch_acc/len(val_loader))\n",
    "                              \n",
    "    \n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}| Val Acc: {val_epoch_acc/len(val_loader):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Trying some stuff with learning rate schedulers__\n",
    "\n",
    "just copied and paster the cell above. Modify the training loop to incorporate https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a lot of this came from: https://towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab\n",
    "\n",
    "print(\"Begin training.\")\n",
    "for e in tqdm(range(1, n_epochs+1)):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    net.train()\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_train_pred = net(X_train_batch)\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch)\n",
    "        train_acc = multi_acc(y_train_pred, y_train_batch)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "        \n",
    "        \n",
    "    # VALIDATION    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_acc = 0\n",
    "        \n",
    "        net.eval()\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            \n",
    "            y_val_pred = net(X_val_batch)\n",
    "                        \n",
    "            val_loss = criterion(y_val_pred, y_val_batch)\n",
    "            val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "            \n",
    "            val_epoch_loss += val_loss.item()\n",
    "            val_epoch_acc += val_acc.item()\n",
    "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "    loss_stats['val'].append(val_epoch_loss/len(val_loader))\n",
    "    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
    "    accuracy_stats['val'].append(val_epoch_acc/len(val_loader))\n",
    "                              \n",
    "    \n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}| Val Acc: {val_epoch_acc/len(val_loader):.3f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
